{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How an LLM Learns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are 2 main types of LLM training:\n",
    "1. Pre-training. (let model learn from the data)\n",
    "2. Supervised fine-tuning. (let model respons how people would like to see it)\n",
    "\n",
    "\n",
    "For our case we will consider the pra-training as first step.\n",
    "\n",
    "You can imagine giant machine with millions of adjustable knobs.\n",
    "\n",
    "Training process consist of this loop:\n",
    "1. **Predict:** Give the model a text snippet, and ask it to guess the next word.\n",
    "2. **Compare:** Compare the predicted word with the actual word.\n",
    "3. **Calculate Error (loss):** Calculate the error between the predicted word and the actual word.\n",
    "4. **Update (Learn):** Slightly adjust all the millions of knobs in a direction that would make the guess less wrong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingredient #1 - The Data and Tokenization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24932\\273828168.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtext_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Hello, how are you?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"google/gemma-3-1b-it\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "text_data = \"Hello, how are you?\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "\n",
    "encoded_text = tokenizer.encode(text_data)\n",
    "print(encoded_text)\n",
    "\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(decoded_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our script, load_dataset and ds.map do this for billions of words. The DataCollatorForLanguageModeling is what will intelligently batch these tokenized sentences together to feed them to the GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingredient #2 - The Model (The 'Brain' Architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the structure of our model's \"brain.\" This is just a blueprint. For our 500M model project, we will define a specific architecture. Here, we'll create a tiny, \"toy\" version. Crucially, when it's first created, all its 'knobs' (parameters) are set to random values. It is completely untrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ingredient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24932\\377194737.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mIngredient\u001b[0m \u001b[1;31m#2 - The Model (The 'Brain' Architecture)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Ingredient' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GemmaConfig, GemmaForCausalLM\n",
    "\n",
    "# Your script has a `Gemma3TextConfig` block. This is that, but much smaller.\n",
    "# The `hidden_size`, `num_hidden_layers`, etc., define the total number of parameters (~500M for us).\n",
    "config = GemmaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=32,          # Our project: ~1536\n",
    "    num_hidden_layers=3,     # Our project: ~18\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=64,\n",
    ")\n",
    "\n",
    "# This line instantiates the model. It's now an object in memory\n",
    "# with millions of randomly initialized floating-point numbers.\n",
    "untrained_model = GemmaForCausalLM(config)\n",
    "print(f\"Created a new, untrained model with {untrained_model.num_parameters():,} parameters.\")\n",
    "print(\"It currently knows nothing. Its 'knowledge' is random noise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input and the ground truth label for one training example\n",
    "input_text = \"The curious cat climbed the\"\n",
    "target_word = \"tall\"\n",
    "\n",
    "# Convert to tensors (the data format GPUs use)\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "# The label is the ID of the *next* token\n",
    "target_id = tokenizer.encode(target_word)[0]\n",
    "\n",
    "# --- STEP 1: PREDICT ---\n",
    "# The model does a \"forward pass\": input data flows through the network's layers.\n",
    "outputs = untrained_model(input_ids)\n",
    "# The output is `logits`: a raw score for every single word in the vocabulary.\n",
    "# We only care about the prediction for the very last token.\n",
    "last_token_logits = outputs.logits[0, -1, :] # [batch_size, sequence_position, vocab_size]\n",
    "\n",
    "# --- STEP 2: COMPARE (Visually) ---\n",
    "# Let's see how wrong the random model is.\n",
    "predicted_token_id = torch.argmax(last_token_logits)\n",
    "print(f\"Input Text:         '{input_text}'\")\n",
    "print(f\"Correct Next Word:  '{target_word}' (Token ID: {target_id})\")\n",
    "print(f\"Model's Prediction: '{tokenizer.decode(predicted_token_id)}' (Token ID: {predicted_token_id.item()})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
